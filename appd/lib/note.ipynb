{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4cc061c",
   "metadata": {},
   "source": [
    "Ce que je doit faire pour ChatGpt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7216af7d",
   "metadata": {},
   "source": [
    "parfait—on passe au “pipeline LLM”. Voici un plan clair, fiable et modulable qui colle à ton app (Flutter + Supabase auth déjà ok).\n",
    "\n",
    "1) Architecture (vue d’ensemble)\n",
    "\n",
    "App Flutter\n",
    "\n",
    "Tu enregistres → tu as un fichier audio local (m4a).\n",
    "\n",
    "Tu crées une note “en cours” dans la DB, upload l’audio dans Supabase Storage, puis tu déclenches un job serveur.\n",
    "\n",
    "Fonction serveur (Supabase Edge Function / Cloud Function)\n",
    "\n",
    "Reçoit note_id, storage_path.\n",
    "\n",
    "Télécharge l’audio (via URL signée).\n",
    "\n",
    "Passe par un moteur STT (ex.: Whisper, Deepgram, etc.) → transcription (texte).\n",
    "\n",
    "Puis appelle un LLM pour générer:\n",
    "\n",
    "“Note” (résumé/points clés)\n",
    "\n",
    "“Quiz” (JSON QCM)\n",
    "\n",
    "“Flashcards” (JSON front/back)\n",
    "\n",
    "Sauvegarde tout en base, met status=done.\n",
    "\n",
    "Retour temps réel côté app\n",
    "\n",
    "L’app s’abonne aux Row Level Changes sur notes (Realtime) et affiche les étapes: uploading → transcribing → generating → done sans rechargement.\n",
    "\n",
    "2) Schéma minimal côté base (Postgres)\n",
    "-- Table des notes\n",
    "create table notes (\n",
    "  id uuid primary key default gen_random_uuid(),\n",
    "  user_id uuid not null references auth.users(id),\n",
    "  title text,\n",
    "  audio_path text,              -- ex: recordings/{userId}/{noteId}.m4a (Storage)\n",
    "  status text not null check (status in ('uploading','transcribing','generating','done','error')),\n",
    "  language text,\n",
    "  duration_secs int,\n",
    "  transcript text,              -- transcription brute\n",
    "  summary_md text,              -- note/résumé en markdown\n",
    "  quiz_json jsonb,              -- [{question, options[], correct_index, explanation}]\n",
    "  flashcards_json jsonb,        -- [{front, back}]\n",
    "  created_at timestamptz default now(),\n",
    "  updated_at timestamptz default now()\n",
    ");\n",
    "create index notes_user_idx on notes(user_id);\n",
    "\n",
    "\n",
    "Bucket storage: recordings (public = non, on utilise des URLs signées).\n",
    "\n",
    "3) Côté Flutter (après “Done”)\n",
    "import 'dart:io';\n",
    "import 'package:uuid/uuid.dart';\n",
    "import 'package:supabase_flutter/supabase_flutter.dart';\n",
    "\n",
    "final supa = Supabase.instance.client;\n",
    "\n",
    "Future<void> processNote(String audioLocalPath) async {\n",
    "  final user = supa.auth.currentUser!;\n",
    "  final noteId = const Uuid().v4();\n",
    "  final storagePath = 'recordings/${user.id}/$noteId.m4a';\n",
    "\n",
    "  // 1) Créer la note en “uploading”\n",
    "  await supa.from('notes').insert({\n",
    "    'id': noteId,\n",
    "    'user_id': user.id,\n",
    "    'title': 'Untitled',\n",
    "    'status': 'uploading',\n",
    "    'audio_path': storagePath,\n",
    "  });\n",
    "\n",
    "  // 2) Uploader l’audio\n",
    "  await supa.storage.from('recordings').upload(\n",
    "    storagePath,\n",
    "    File(audioLocalPath),\n",
    "    fileOptions: const FileOptions(contentType: 'audio/m4a'),\n",
    "  );\n",
    "\n",
    "  // 3) Lancer le job serveur\n",
    "  await supa.functions.invoke('process-note', body: {\n",
    "    'note_id': noteId,\n",
    "    'audio_path': storagePath,\n",
    "  });\n",
    "\n",
    "  // 4) S’abonner aux changements pour l’UI\n",
    "  supa.channel('notes-$noteId')\n",
    "    .onPostgresChanges(\n",
    "      event: PostgresChangeEvent.update,\n",
    "      schema: 'public',\n",
    "      table: 'notes',\n",
    "      filter: PostgresChangeFilter.equals('id', noteId),\n",
    "      callback: (payload) {\n",
    "        final row = payload.newRecord;\n",
    "        // row['status'] => 'transcribing' | 'generating' | 'done' | 'error'\n",
    "        // maj UI ici\n",
    "      },\n",
    "    )\n",
    "    .subscribe();\n",
    "}\n",
    "\n",
    "\n",
    "Bonus UX: si l’upload est long, affiche une barre de progression; après done, navige vers l’écran qui affiche résumé/quiz/flashcards.\n",
    "\n",
    "4) Fonction serveur (Edge Function – pseudo-code Deno)\n",
    "\n",
    "Avantages: clé LLM côté serveur (sécurisée), conversions, gestion d’erreurs centralisée.\n",
    "\n",
    "// supabase/functions/process-note/index.ts\n",
    "import 'jsr:@supabase/functions'; // runtime Deno\n",
    "import { createClient } from 'https://esm.sh/@supabase/supabase-js@2';\n",
    "\n",
    "const OPENAI_KEY = Deno.env.get('OPENAI_API_KEY')!; // ou autre provider STT/LLM\n",
    "const SUPABASE_URL = Deno.env.get('SUPABASE_URL')!;\n",
    "const SUPABASE_ANON = Deno.env.get('SUPABASE_ANON_KEY')!; // ou service_role si besoin\n",
    "\n",
    "Deno.serve(async (req) => {\n",
    "  const { note_id, audio_path } = await req.json();\n",
    "  const supa = createClient(SUPABASE_URL, SUPABASE_ANON, {\n",
    "    global: { headers: { Authorization: req.headers.get('Authorization')! } }\n",
    "  });\n",
    "\n",
    "  // 0) Guard\n",
    "  if (!note_id || !audio_path) return new Response('Bad request', { status: 400 });\n",
    "\n",
    "  try {\n",
    "    // A) status -> transcribing\n",
    "    await supa.from('notes').update({ status: 'transcribing' }).eq('id', note_id);\n",
    "\n",
    "    // B) URL signée pour l’audio\n",
    "    const { data: signed } = await supa.storage.from('recordings')\n",
    "      .createSignedUrl(audio_path, 60 * 10); // 10 min\n",
    "    const audioUrl = signed?.signedUrl!;\n",
    "    const audioResp = await fetch(audioUrl);\n",
    "    const audioBlob = new Blob([await audioResp.arrayBuffer()], { type: 'audio/m4a' });\n",
    "\n",
    "    // C) Transcription (ex: Whisper / autre STT)\n",
    "    const form = new FormData();\n",
    "    form.append('file', audioBlob, 'audio.m4a');\n",
    "    form.append('model', 'whisper-1');              // adapte selon ton fournisseur\n",
    "    form.append('response_format', 'verbose_json'); // si dispo (lang, duration)\n",
    "    const stt = await fetch('https://api.openai.com/v1/audio/transcriptions', {\n",
    "      method: 'POST',\n",
    "      headers: { Authorization: `Bearer ${OPENAI_KEY}` },\n",
    "      body: form,\n",
    "    }).then(r => r.json());\n",
    "\n",
    "    const transcript: string = stt.text ?? '';\n",
    "    const language: string | undefined = stt.language;\n",
    "    const durationSec: number | undefined = stt.duration;\n",
    "\n",
    "    // D) status -> generating\n",
    "    await supa.from('notes').update({\n",
    "      status: 'generating',\n",
    "      transcript,\n",
    "      language,\n",
    "      duration_secs: durationSec ? Math.round(durationSec) : null,\n",
    "    }).eq('id', note_id);\n",
    "\n",
    "    // E) Générations LLM\n",
    "    const summaryPrompt = `\n",
    "Tu es un preneur de notes. Résume clairement l'audio ci-dessous en Markdown :\n",
    "- titre\n",
    "- 5–8 bullet points\n",
    "- \"Key takeaways\" à la fin.\n",
    "Texte:\n",
    "\"\"\"${transcript}\"\"\"`;\n",
    "\n",
    "    const quizPrompt = `\n",
    "Génère 6 questions QCM en JSON compact:\n",
    "[{\"question\":\"\",\"options\":[\"A\",\"B\",\"C\",\"D\"],\"correct_index\":0,\"explanation\":\"\"}, ...]\n",
    "Le contenu doit venir uniquement du texte suivant:\n",
    "\"\"\"${transcript}\"\"\"`;\n",
    "\n",
    "    const flashPrompt = `\n",
    "Crée 12 flashcards JSON:\n",
    "[{\"front\":\"\", \"back\":\"\"}, ...]\n",
    "À partir du texte:\n",
    "\"\"\"${transcript}\"\"\"`;\n",
    "\n",
    "    const askLLM = async (prompt: string) => {\n",
    "      const resp = await fetch('https://api.openai.com/v1/chat/completions', {\n",
    "        method: 'POST',\n",
    "        headers: {\n",
    "          Authorization: `Bearer ${OPENAI_KEY}`,\n",
    "          'Content-Type': 'application/json',\n",
    "        },\n",
    "        body: JSON.stringify({\n",
    "          model: 'gpt-4o-mini', // au choix\n",
    "          messages: [{ role: 'user', content: prompt }],\n",
    "          temperature: 0.3,\n",
    "        }),\n",
    "      }).then(r => r.json());\n",
    "      return resp.choices?.[0]?.message?.content ?? '';\n",
    "    };\n",
    "\n",
    "    const [summaryMd, quizJsonStr, flashJsonStr] = await Promise.all([\n",
    "      askLLM(summaryPrompt),\n",
    "      askLLM(quizPrompt),\n",
    "      askLLM(flashPrompt),\n",
    "    ]);\n",
    "\n",
    "    // F) Save & done\n",
    "    await supa.from('notes').update({\n",
    "      status: 'done',\n",
    "      summary_md: summaryMd,\n",
    "      quiz_json: JSON.parse(quizJsonStr),\n",
    "      flashcards_json: JSON.parse(flashJsonStr),\n",
    "    }).eq('id', note_id);\n",
    "\n",
    "    return new Response('ok');\n",
    "  } catch (e) {\n",
    "    console.error(e);\n",
    "    await supa.from('notes').update({ status: 'error' }).eq('id', note_id);\n",
    "    return new Response('error', { status: 500 });\n",
    "  }\n",
    "});\n",
    "\n",
    "\n",
    "Tu peux remplacer OpenAI par n’importe quel provider STT/LLM. L’important: clé uniquement côté serveur.\n",
    "\n",
    "5) Écrans côté app\n",
    "\n",
    "Écran “Note en cours” : montre statut + spinner + message d’étape.\n",
    "\n",
    "Écran “Note” : onglets Summary | Transcript | Quiz | Flashcards.\n",
    "\n",
    "Quiz: parse quiz_json et affiche QCM avec corrections.\n",
    "\n",
    "Flashcards: flip-cards à partir de flashcards_json.\n",
    "\n",
    "Transcript: texte scrollable + recherche.\n",
    "\n",
    "6) Détails pratiques / pièges\n",
    "\n",
    "Durée/poids: impose des limites (ex. ≤ 60 min / ≤ 50 MB).\n",
    "\n",
    "Formats audio: m4a (AAC) est OK. Si nécessaire, convertis en .wav côté serveur avant STT.\n",
    "\n",
    "Langue: tu peux détecter la langue via STT et ajuster les prompts (ex.: “réponds en fr”).\n",
    "\n",
    "Erreurs: garde status=error + error_message pour informer l’utilisateur.\n",
    "\n",
    "Sécurité: bucket private, requêtes Edge fonctionnent avec JWT de l’utilisateur pour contrôler l’accès.\n",
    "\n",
    "Coûts: découpe longue transcription en segments (10–15 min) et concatène.\n",
    "\n",
    "Scalabilité: si le trafic monte, passe par une queue (table jobs) et un cron/worker.\n",
    "\n",
    "7) Ce que tu peux faire tout de suite\n",
    "\n",
    "Crée la table notes et le bucket recordings.\n",
    "\n",
    "Implémente le flux Flutter (insert → upload → invoke → subscribe).\n",
    "\n",
    "Déploie process-note avec ta clé STT/LLM en secret.\n",
    "\n",
    "Ajoute l’écran Note pour lire summary_md, quiz_json, flashcards_json.\n",
    "\n",
    "Quand tu as le bucket + la fonction en place, on branche ton écran et on peaufine les prompts (style, longueur, niveau du quiz, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43b111c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e7fa9c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
